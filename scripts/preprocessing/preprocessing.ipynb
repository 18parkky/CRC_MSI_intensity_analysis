{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "You will be able to access the raw sequencing data used in this study by providing the authors with a valid request.\n",
    "After downloading the raw sequencing files, \n",
    "\n",
    "After receiving the raw sequencing files, we used CellRanger (8.0.1) to obtain gene expression data and barcoded BAM files.\n",
    "Then, we ran Scrublet for *each* sample (which is best practice according to Scrublet), then merged the resulting h5ad files into one.\n",
    "Note that for Chen et al. dataset, you must input multiple FASTQ files according to their sample IDs for each sample.\n",
    "Below is are examples of the command lines used:\n",
    "    ~/cellranger-8.0.1/bin/cellranger count --id {id} --transcriptome ~/refdata-gex-GRCh38-2020-A --fastqs {DIR_TO_FASTQ} --create-bam true\n",
    "    ~/cellranger-8.0.1/bin/cellranger count --id {id} --transcriptome ~/refdata-gex-GRCh38-2020-A \\ \n",
    "            --fastqs {DIR_TO_FASTQ_1},{DIR_TO_FASTQ_2},{DIR_TO_FASTQ_3},{DIR_TO_FASTQ_4} \\ \n",
    "            --create-bam true --nosecondary\n",
    "    \n",
    "Then, as we will see below, \n",
    "    (1) the resulting h5ad files will be preprocessed using the standard Scanpy workflow\n",
    "    (2) releveant metadata (e.g., patient, author-provided cell type, etc) will be appended to Scanpy object.\n",
    "    (3) and MSI profile information will be appended to the Scanpy object.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY_OUT = '/node200data/18parkky/datasets/data/public/processed_data/CRC_MSI_intensity_analysis_clean_data'\n",
    "\n",
    "PATH_TO_KINKER_ADATA    = '/node200data/18parkky/datasets/data/public/processed_data/Kinker_et_al/CPM_data.metalabeled.h5ad'\n",
    "PATH_TO_CHEN_ADATA      = '/node200data/18parkky/datasets/data/public/processed_data/Immunotherapy_CRC_Chen_et_al/1_raw_h5ad/Chen.MSI.raw.h5ad'\n",
    "PATH_TO_JOANITO_ADATA   = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/1_raw_h5ad/Joanito.raw.h5ad'\n",
    "\n",
    "PATH_TO_CHEN_METADATA   = '/node200data/18parkky/datasets/data/public/processed_data/Immunotherapy_CRC_Chen_et_al/AuthorProcessedData/GSE236581_CRC-ICB_metadata.txt.gz'\n",
    "PATH_TO_CHEN_METADATA2  = '/node200data/18parkky/datasets/data/public/FASTQ/Immunotherapy_CRC_Chen_et_al/metadata/Chen_et_al.MSI.CRC.woSD.run_meta.organized.tsv'\n",
    "\n",
    "PATH_TO_KINKER_METADATA = '/node200data/18parkky/datasets/data/public/processed_data/Kinker_et_al/Metadata.txt'\n",
    "PATH_TO_KINKER_NANOMNT  = '/node200data/18parkky/datasets/data/public/processed_data/Kinker_et_al/AlleleTable.merged.tsv.gz'\n",
    "\n",
    "PATH_TO_JOANITO_METADATA_EPI    = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/CRC-SG1/synapse-metadata/Epithelial_metadata.csv'\n",
    "PATH_TO_JOANITO_METADATA_NONEPI = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/CRC-SG1/synapse-metadata/NonEpithelial_metadata.csv'\n",
    "PATH_TO_JOANITO_METADATA2       = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/CRC-SG1/synapse-metadata/patient_clinical_information.csv'\n",
    "PATH_TO_JOANITO_SG1_METADATA    = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/CRC-SG1/EGAD00001008555-metadata/organized_metadata.tsv'\n",
    "PATH_TO_JOANITO_KUL_METADATA    = '/node200data/18parkky/datasets/data/public/processed_data/Joanito_et_al/KUL/organized_metadata.tsv'\n",
    "\n",
    "DIRECTORY_CHEN_CELLRANGER_OUT       = '/node200data/18parkky/datasets/data/public/BAM/Immunotherapy_CRC_Chen_et_al/MSI/CellRangerOut' \n",
    "DIRECTORY_JOANITO_CELLRANGER_OUT    = '/node200data/18parkky/datasets/data/public/BAM/Joanito_et_al/CellRangerOut'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessScanpy(adata, batch_key, random_state=42):\n",
    "    \n",
    "    adata.obs_names_make_unique()\n",
    "    adata.var_names_make_unique()\n",
    "        \n",
    "    sc.pp.filter_cells(adata, min_genes=300)\n",
    "    sc.pp.filter_genes(adata, min_cells=10)\n",
    "    \n",
    "    adata.raw = adata.copy()\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "    sc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=batch_key)\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    \n",
    "    sc.pp.scale(adata, max_value=10)\n",
    "    sc.tl.pca(adata, svd_solver=\"arpack\")\n",
    "    \n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=40)\n",
    "    sc.tl.umap(adata, random_state=random_state)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "''' \n",
    "The following function assumes you ran NanoMnT getAlleleTable using possorted_genome_bam.bam and saved the results in ~/{SampleID}/outs/possorted_genome_bam.STR_allele_table.tsv.\n",
    "'''\n",
    "def preprocessNanoMnT( set_SampleIDs, DIRECTORY_TO_CELLRANGER_OUT ):\n",
    "    results = list()\n",
    "\n",
    "    # for SampleID, edf in metadata[(metadata['BiopsySite']=='Tumor') & (metadata['TreatmentStage']=='Pre')].groupby(\"SampleID\"):\n",
    "    for SampleID in set_SampleIDs:\n",
    "\n",
    "        PATH_alleleTable = f'{DIRECTORY_TO_CELLRANGER_OUT}/{SampleID}/outs/possorted_genome_bam.STR_allele_table.tsv'\n",
    "        PATH_processed_alleleTable = f'{DIRECTORY_TO_CELLRANGER_OUT}/{SampleID}/outs/possorted_genome_bam.STR_allele_table.preprocessed.tsv'\n",
    "    \n",
    "        AlleleTable = pd.read_csv(PATH_alleleTable, sep='\\t')\n",
    "        AlleleTable['SampleID'] = SampleID\n",
    "\n",
    "        ### 1. Filter out low-quality flankings (e.g., indels within flankings)        \n",
    "        col_flanking_quality = list()\n",
    "        for tup2 in AlleleTable.itertuples():\n",
    "            bf = f'{tup2.left_flanking_seq}{tup2.right_flanking_seq}'\n",
    "            if '*' in bf:\n",
    "                col_flanking_quality.append( 'Poor' )\n",
    "            elif bf.upper() != bf:\n",
    "                col_flanking_quality.append( 'Poor' )\n",
    "            else:\n",
    "                col_flanking_quality.append( 'Good' )\n",
    "                \n",
    "        AlleleTable['flanking_quality'] = col_flanking_quality\n",
    "        AlleleTable = AlleleTable[(AlleleTable['flanking_quality']=='Good')].copy()\n",
    "        \n",
    "        ### 2. Filter out G/C repeats\n",
    "        AlleleTable = AlleleTable[(AlleleTable['repeat_unit'].isin(['A', 'T']))].copy()\n",
    "        \n",
    "        ### 3. Filter out reads without CB or UMI\n",
    "        AlleleTable.dropna(inplace=True,)\n",
    "        AlleleTable = AlleleTable[AlleleTable['reference_STR_allele']<=24].copy()\n",
    "        \n",
    "        AlleleTable['diff'] = AlleleTable['read_STR_allele'] - AlleleTable['reference_STR_allele']\n",
    "        AlleleTable.to_csv(PATH_processed_alleleTable, sep='\\t', index=False)\n",
    "        results.append(AlleleTable)\n",
    "\n",
    "    results = pd.concat(results)        \n",
    "    results.reset_index(inplace=True, drop=True)\n",
    "    results['Identifier'] = [ f'{tup.SampleID}-{tup.CB}' for tup in results.itertuples() ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "def saveWithPickle(obj, PATH_out, filename=\"saveWithPickle\"):\n",
    "    with open(f'{PATH_out}/{filename}.pickle', 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def loadFromPickle(dir_pickle):\n",
    "    with open(dir_pickle, 'rb') as handle:\n",
    "        unserialized_pickle = pickle.load(handle)\n",
    "    return unserialized_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18parkky/anaconda3-2021.11/envs/sc/lib/python3.12/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "adata_k = sc.read_h5ad(PATH_TO_KINKER_ADATA)    # Kinker et al\n",
    "adata_c = sc.read_h5ad(PATH_TO_CHEN_ADATA)      # Chen et al\n",
    "adata_j = sc.read_h5ad(PATH_TO_JOANITO_ADATA)   # Joanito et al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [0] Doublet removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18parkky/anaconda3-2021.11/envs/sc/lib/python3.12/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "# Doublet removal\n",
    "doublet_score_threshold = np.mean( adata_j.obs['doublet_score'] ) + np.std( adata_j.obs['doublet_score'] )\n",
    "adata_j = adata_j[adata_j.obs['doublet_score']<=doublet_score_threshold].copy()\n",
    "\n",
    "doublet_score_threshold = np.mean( adata_c.obs['doublet_score'] ) + np.std( adata_c.obs['doublet_score'] )\n",
    "adata_c = adata_c[adata_c.obs['doublet_score']<=doublet_score_threshold].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Appending metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_291103/2915129355.py:2: DtypeWarning: Columns (4,9,10,11,12,13,14,15,16,17,18,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata_k = pd.read_csv(PATH_TO_KINKER_METADATA, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Kinker et al - Cell line, Pooling ID, Type of cancer\n",
    "metadata_k = pd.read_csv(PATH_TO_KINKER_METADATA, sep='\\t')\n",
    "dict_CB_to_MetaData = { tup.NAME : [tup.Cell_line, tup.Pool_ID, tup.Cancer_type] for tup in metadata_k.itertuples() }\n",
    "for CB in adata_k.obs['CB']:\n",
    "    try: dict_CB_to_MetaData[CB]\n",
    "    except KeyError: dict_CB_to_MetaData[CB] = [None, None, None]\n",
    "adata_k.obs['CellLine'] = [ dict_CB_to_MetaData[CB][0] for CB in adata_k.obs['CB'] ]\n",
    "adata_k.obs['PoolID'] = [ dict_CB_to_MetaData[CB][1] for CB in adata_k.obs['CB'] ]\n",
    "adata_k.obs['CancerType'] = [ dict_CB_to_MetaData[CB][2] for CB in adata_k.obs['CB'] ]\n",
    "\n",
    "adata_k.obs['Identifier'] = [ f'{tup.CB.split(\"-\")[0]}-{tup.PoolID}' for tup in adata_k.obs.itertuples() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chen et al - Unique identifier, Author-annotated cell type (broad-lvl1 and specific-lvl2)\n",
    "metadata_c = pd.read_csv(PATH_TO_CHEN_METADATA, sep=' ')\n",
    "metadata_c['CB'] = [ idx.split(\"_\")[-1] for idx in metadata_c.index ]\n",
    "metadata_c['SampleID']      = [ tup.Ident.replace('CRC', 'P') for tup in metadata_c.itertuples() ]\n",
    "metadata_c['Identifier']    = [ f'{tup.SampleID}-{tup.CB}' for tup in metadata_c.itertuples() ] # Identifier unique for each cell\n",
    "\n",
    "adata_c.obs['Identifier'] = [ f'{tup.SampleID}-{tup.Index.split(\"-\")[0]}' for tup in adata_c.obs.itertuples() ]\n",
    "\n",
    "dict_Identifier_to_Celltypes = { tup.Identifier : [tup.MajorCellType, tup.SubCellType] for tup in metadata_c.itertuples() }\n",
    "\n",
    "col_MajorCellType, col_SubCellType = list(), list()\n",
    "for tup in adata_c.obs.itertuples():\n",
    "    try:\n",
    "        CellTypes = dict_Identifier_to_Celltypes[tup.Identifier]\n",
    "        col_MajorCellType.append( CellTypes[0] )\n",
    "        col_SubCellType.append( CellTypes[1] )\n",
    "    except KeyError:\n",
    "        col_MajorCellType.append( 'N/A' )\n",
    "        col_SubCellType.append( 'N/A' )\n",
    "        \n",
    "adata_c.obs['Author_CellType_lvl_1'] = col_MajorCellType\n",
    "adata_c.obs['Author_CellType_lvl_2'] = col_SubCellType\n",
    "adata_c = adata_c[adata_c.obs['Author_CellType_lvl_1']!='N/A'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18parkky/anaconda3-2021.11/envs/sc/lib/python3.12/site-packages/anndata/_core/anndata.py:1818: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "# Joanito et al - \n",
    "metadata_j_epi    = pd.read_csv(PATH_TO_JOANITO_METADATA_EPI)\n",
    "metadata_j_nonepi = pd.read_csv(PATH_TO_JOANITO_METADATA_NONEPI)\n",
    "metadata_j = pd.concat([ metadata_j_epi, metadata_j_nonepi ])\n",
    "\n",
    "metadata_j = metadata_j[(metadata_j['patient.ID'].isin( set(adata_j.obs['PatientID']) ))].copy()\n",
    "metadata_j.rename({'sample.origin' : 'BiopsySite'}, axis=1, inplace=True)\n",
    "metadata_j['CB'] = [ cellID.split(\"_\")[-1].split(\"-\")[0] for cellID in metadata_j['cell.ID'] ]\n",
    "\n",
    "dict_MUXID_to_SampleID = {  'MUX8579': 'XHC102',\n",
    "                            'MUX8580': 'XHC103',\n",
    "                            'MUX8581': 'XHC104',\n",
    "                            'MUX8582': 'XHC105',\n",
    "                            'MUX8583': 'XHC106',\n",
    "                            'MUX8584': 'XHC107',\n",
    "                            \n",
    "                            'MUX8815': 'XHC129',\n",
    "                            'MUX8816': 'XHC130',\n",
    "                            'MUX8817': 'XHC131',\n",
    "                            'MUX9005': 'XHC134',\n",
    "                            'MUX9006': 'XHC135',\n",
    "                            'MUX9007': 'XHC136',\n",
    "                            'MUX9008': 'XHC137',\n",
    "                            'MUX9009': 'XHC138',\n",
    "                            'MUX9010': 'XHC139',\n",
    "                            'MUX9322': 'XHC154',\n",
    "                            'MUX9380': 'XHC166',\n",
    "                             \n",
    "                            'MUX9011': 'XHC140',\n",
    "                            }\n",
    "\n",
    "col_SampleID = list()\n",
    "for tup in metadata_j.itertuples():\n",
    "    try:\n",
    "        col_SampleID.append(dict_MUXID_to_SampleID[list(tup)[5]])\n",
    "    except KeyError:\n",
    "        col_SampleID.append( list(tup)[5] )\n",
    "        \n",
    "metadata_j['SampleID'] = col_SampleID\n",
    "metadata_j['Identifier'] = [ f'{tup.SampleID}-{tup.CB}' for tup in metadata_j.itertuples() ]\n",
    "\n",
    "adata_j.obs['Identifier'] = [ f'{tup.SampleID}-{tup.Index.split(\"-\")[0]}' for tup in adata_j.obs.itertuples() ]\n",
    "dict_Identifier_to_Celltypes = { tup.Identifier : list(tup)[-6] for tup in metadata_j.itertuples() }\n",
    "\n",
    "col_MajorCellType = list()\n",
    "for tup in adata_j.obs.itertuples():\n",
    "    try:\n",
    "        CellTypes = dict_Identifier_to_Celltypes[tup.Identifier]\n",
    "        col_MajorCellType.append( CellTypes )\n",
    "    except KeyError:\n",
    "        col_MajorCellType.append( 'N/A' )\n",
    "        \n",
    "adata_j.obs['Author_CellType_lvl_1'] = col_MajorCellType\n",
    "adata_j.obs['Author_CellType_lvl_2'] = 'N/A'\n",
    "adata_j = adata_j[adata_j.obs['Author_CellType_lvl_1']!='N/A'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Scanpy Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/18parkky/anaconda3-2021.11/envs/sc/lib/python3.12/site-packages/scanpy/preprocessing/_scale.py:316: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    }
   ],
   "source": [
    "adata_j = preprocessScanpy(adata_j, batch_key='SampleID', random_state=0)  \n",
    "adata_c = preprocessScanpy(adata_c, batch_key='SampleID', random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Appending NanoMnT results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chen et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_291103/362435411.py:6: DtypeWarning: Columns (15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  NanoMnT_results_c = pd.read_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Chen_et_al.tsv.gz', sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# [1] Collect NanoMnT results and store into dictionary\n",
    "metadata_c2 = pd.read_csv(PATH_TO_CHEN_METADATA2, sep='\\t')\n",
    "NanoMnT_results_c = preprocessNanoMnT( set(metadata_c2[(metadata_c2['TreatmentStage']=='Pre')]['SampleID']), \n",
    "                                            DIRECTORY_CHEN_CELLRANGER_OUT )\n",
    "NanoMnT_results_c.to_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Chen_et_al.tsv.gz', sep='\\t', index=False, compression='gzip')\n",
    "NanoMnT_results_c = pd.read_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Chen_et_al.tsv.gz', sep='\\t')\n",
    "\n",
    "dict_Identifier_to_MSprofile_c = dict()\n",
    "\n",
    "for Identifier, edf in NanoMnT_results_c.groupby(\"Identifier\"):\n",
    "    edf_o = edf['diff'].dropna()\n",
    "    if len(edf_o) > 0:\n",
    "        dict_Identifier_to_MSprofile_c[Identifier] = [ np.mean(edf_o), np.std(edf_o), len(edf_o) ]\n",
    "        \n",
    "for Identifier in adata_c.obs['Identifier']:\n",
    "    try: dict_Identifier_to_MSprofile_c[Identifier]\n",
    "    except KeyError: dict_Identifier_to_MSprofile_c[Identifier]=[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] Overlay microsatellite information to Scanpy object\n",
    "adata_c.obs['AvgSTRDiff'] = [ dict_Identifier_to_MSprofile_c[Identifier][0] for Identifier in adata_c.obs['Identifier'] ]\n",
    "adata_c.obs['StdSTRDiff'] = [ dict_Identifier_to_MSprofile_c[Identifier][1] for Identifier in adata_c.obs['Identifier'] ]\n",
    "adata_c.obs['NumSTRLoci'] = [ dict_Identifier_to_MSprofile_c[Identifier][2] for Identifier in adata_c.obs['Identifier'] ]\n",
    "adata_c.obs['MSI_score']  = -1 * adata_c.obs['AvgSTRDiff'] * adata_c.obs['StdSTRDiff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joanito et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Collect NanoMnT results and store into dictionary\n",
    "# NanoMnT_results_j = preprocessNanoMnT( set(adata_j.obs['SampleID']), DIRECTORY_JOANITO_CELLRANGER_OUT )\n",
    "# NanoMnT_results_j.to_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Joanito_et_al.tsv.gz', sep='\\t', index=False, compression='gzip')\n",
    "NanoMnT_results_j = pd.read_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Joanito_et_al.tsv.gz', sep='\\t')\n",
    "\n",
    "dict_Identifier_to_MSprofile_j = dict()\n",
    "\n",
    "for Identifier, edf in NanoMnT_results_j.groupby(\"Identifier\"):\n",
    "    edf_o = edf['diff'].dropna()\n",
    "    if len(edf_o) > 0:\n",
    "        dict_Identifier_to_MSprofile_j[Identifier] = [ np.mean(edf_o), np.std(edf_o), len(edf_o) ]\n",
    "\n",
    "for Identifier in adata_j.obs['Identifier']:\n",
    "    try: dict_Identifier_to_MSprofile_j[Identifier]\n",
    "    except KeyError: dict_Identifier_to_MSprofile_j[Identifier]=[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] Overlay microsatellite information to Scanpy object\n",
    "adata_j.obs['AvgSTRDiff'] = [ dict_Identifier_to_MSprofile_j[Identifier][0] for Identifier in adata_j.obs['Identifier'] ]\n",
    "adata_j.obs['StdSTRDiff'] = [ dict_Identifier_to_MSprofile_j[Identifier][1] for Identifier in adata_j.obs['Identifier'] ]\n",
    "adata_j.obs['NumSTRLoci'] = [ dict_Identifier_to_MSprofile_j[Identifier][2] for Identifier in adata_j.obs['Identifier'] ]\n",
    "adata_j.obs['MSI_score']  = -1 * adata_j.obs['AvgSTRDiff'] * adata_j.obs['StdSTRDiff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinker et al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Collect NanoMnT results and store into dictionary\n",
    "''' \n",
    "Unlike Joanito et al. and Chen et al. datasets, you don't need to run CellRanger for Kinker et al. dataset.\n",
    "Instead, BAM files are available for download at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE157220 \n",
    "You can directly run NanoMnT getAlleleTable on these BAM files and merge them. \n",
    "'''\n",
    "NanoMnT_results_k = pd.read_csv(PATH_TO_KINKER_NANOMNT, sep='\\t')\n",
    "NanoMnT_results_k['PoolID'] = [ str(pi.split('_')[1]) for pi in NanoMnT_results_k['pool_id'] ]\n",
    "NanoMnT_results_k['Identifier'] = [ f'{tup.CB.split(\"-\")[0]}-{tup.PoolID}' for tup in NanoMnT_results_k.itertuples() ]\n",
    "NanoMnT_results_k.to_csv(f'{DIRECTORY_OUT}/NanoMnT.AlleleTable.Kinker_et_al.tsv.gz', sep='\\t', index=False, compression='gzip')\n",
    "\n",
    "col_flanking_quality = list()\n",
    "for tup2 in NanoMnT_results_k.itertuples():\n",
    "    bf = f'{tup2.left_flanking_seq}{tup2.right_flanking_seq}'\n",
    "    if '*' in bf:\n",
    "        col_flanking_quality.append( 'Poor' )\n",
    "    elif bf.upper() != bf:\n",
    "        col_flanking_quality.append( 'Poor' )\n",
    "    else:\n",
    "        col_flanking_quality.append( 'Good' )\n",
    "NanoMnT_results_k['flanking_quality'] = col_flanking_quality\n",
    "NanoMnT_results_k = NanoMnT_results_k[(NanoMnT_results_k['flanking_quality']=='Good')].copy()\n",
    "\n",
    "NanoMnT_results_k = NanoMnT_results_k[(NanoMnT_results_k['repeat_unit'].isin(['A', 'T']))].copy()\n",
    "NanoMnT_results_k.dropna(inplace=True,)\n",
    "NanoMnT_results_k = NanoMnT_results_k[NanoMnT_results_k['reference_STR_allele']<=24].copy()\n",
    "\n",
    "NanoMnT_results_k['diff'] = NanoMnT_results_k['read_STR_allele'] - NanoMnT_results_k['reference_STR_allele']\n",
    "NanoMnT_results_k.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dict_Identifier_to_MSprofile_k = dict()\n",
    "\n",
    "for Identifier, edf in NanoMnT_results_k.groupby(\"Identifier\"):\n",
    "    edf_o = edf['diff'].dropna()\n",
    "    if len(edf_o) > 0:\n",
    "        dict_Identifier_to_MSprofile_k[Identifier] = [ np.mean(edf_o), np.std(edf_o), len(edf_o) ]\n",
    "        \n",
    "for Identifier in adata_k.obs['Identifier']:\n",
    "    try: dict_Identifier_to_MSprofile_k[Identifier]\n",
    "    except KeyError: dict_Identifier_to_MSprofile_k[Identifier]=[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] Overlay microsatellite information to Scanpy object\n",
    "adata_k.obs['AvgSTRDiff'] = [ dict_Identifier_to_MSprofile_k[Identifier][0] for Identifier in adata_k.obs['Identifier'] ]\n",
    "adata_k.obs['StdSTRDiff'] = [ dict_Identifier_to_MSprofile_k[Identifier][1] for Identifier in adata_k.obs['Identifier'] ]\n",
    "adata_k.obs['NumSTRLoci'] = [ dict_Identifier_to_MSprofile_k[Identifier][2] for Identifier in adata_k.obs['Identifier'] ]\n",
    "adata_k.obs['MSI_score']  = -1 * adata_k.obs['AvgSTRDiff'] * adata_k.obs['StdSTRDiff']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_k.write(f'{DIRECTORY_OUT}/adata_k.preprocessed.NanoMnT.h5ad')\n",
    "adata_c.write(f'{DIRECTORY_OUT}/adata_c.preprocessed.NanoMnT.h5ad')\n",
    "adata_j.write(f'{DIRECTORY_OUT}/adata_j.preprocessed.NanoMnT.h5ad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
